# ================================================
# RTX 50シリーズ（5090/5070 Ti等）対応版
# CUDA 12.8 + PyTorch Nightly使用
# Codex CLI と 既存のOllama を連携
# ================================================

services:
  # メインの計算化学・ML研究環境
  research-env:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        - BUILDKIT_INLINE_CACHE=1
    container_name: comp-chem-ml-env
    hostname: research-env
    
    # GPU設定（RTX 50シリーズ用）
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    
    # 環境変数（RTX 50シリーズ最適化 + 既存Ollama連携）
    environment:
      # GPU設定
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - CUDA_VISIBLE_DEVICES=0  # 必要に応じて変更
      - TORCH_CUDA_ARCH_LIST=9.0;12.0  # sm_90 (RTX 40) + sm_120 (RTX 50)
      - CUDA_LAUNCH_BLOCKING=0  # パフォーマンス最適化
      - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
      
      # Ollama設定（既存のollamaコンテナに接続）
      - OLLAMA_HOST=http://ollama:11434
      - OLLAMA_API_BASE=http://ollama:11434/v1
      - OLLAMA_MODEL=gpt-oss:20b
      - OPENAI_API_KEY=dummy  # Codexが要求する場合があるため残す
      
      # サービス設定
      - JUPYTER_TOKEN=research2025  # 変更推奨
      - MCP_SERVER_PORT=9121
      
      # Python設定
      - PYTHONPATH=/workspace
      - PYTHONDONTWRITEBYTECODE=1
      - PYTHONUNBUFFERED=1
      
      # その他
      - TZ=Asia/Tokyo
      - LANG=en_US.UTF-8
      - LC_ALL=en_US.UTF-8
    
    # ポートマッピング
    ports:
      - "8888:8888"    # JupyterLab
      - "9121:9121"    # Serena-MCP SSE
      - "9122:9122"    # Serena Dashboard
    
    # ボリュームマウント
    volumes:
      # 作業ディレクトリ（永続化）
      - ./workspace:/workspace
      
      # 設定ファイル（永続化）
      - ./config/serena:/root/.serena
      
      # データセット用
      - ./datasets:/datasets
      
      # モデル保存用
      - ./models:/models
      
      # ログ
      - ./logs:/workspace/logs
      
      # Jupyterノートブック
      - ./notebooks:/workspace/notebooks
      
      # キャッシュ（高速化のため）
      - pip_cache:/root/.cache/pip
      - torch_cache:/root/.cache/torch
      - huggingface_cache:/root/.cache/huggingface
      
      # Dockerソケット（必要に応じて）
      # - /var/run/docker.sock:/var/run/docker.sock
    
    # ネットワーク設定（重要：既存のollamaネットワークに参加）
    networks:
      - research-network      # 内部ネットワーク
      - ai-setup_default      # 既存のOllamaネットワークに参加
    
    # セキュリティ設定
    security_opt:
      - seccomp:unconfined
      - apparmor:unconfined
    
    # リソース制限（RTX 5090用に調整）
    # deploy:
    #   resources:
    #     limits:
    #       cpus: '32'  # RTX 5090向けハイスペック構成
    #       memory: 128G
    #     reservations:
    #       cpus: '16'
    #       memory: 64G
    
    # 再起動ポリシー
    restart: unless-stopped
    
    # ヘルスチェック
    healthcheck:
      test: |
        python3 -c "import torch; assert torch.cuda.is_available()" && \
        curl -f http://localhost:8888/api || exit 1
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    
    # 追加のランタイム設定
    runtime: nvidia  # NVIDIA Dockerランタイムを明示的に指定
    shm_size: '32gb'  # 共有メモリサイズ（大規模モデル用に増量）
    ulimits:
      memlock:
        soft: -1
        hard: -1
      stack:
        soft: 67108864
        hard: 67108864
    
    # 依存関係
    depends_on:
      - gpu-check

  # GPU互換性チェックサービス
  gpu-check:
    image: nvidia/cuda:12.8.0-base-ubuntu22.04
    container_name: gpu-compatibility-check
    command: |
      sh -c "
        nvidia-smi && \
        echo '✅ NVIDIA GPU detected' && \
        echo 'GPU Info:' && \
        nvidia-smi --query-gpu=name,compute_cap,memory.total --format=csv,noheader && \
        echo '' && \
        echo 'Checking for sm_120 (RTX 50 series) support...' && \
        nvidia-smi --query-gpu=compute_cap --format=csv,noheader | grep -q '12.0' && \
        echo '✅ RTX 50 series (sm_120) detected!' || \
        echo '⚠️  No RTX 50 series detected, but continuing...'
      "
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    networks:
      - research-network
    restart: "no"

# ネットワーク定義
networks:
  # 内部研究用ネットワーク
  research-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/24
          gateway: 172.20.0.1
  
  # 既存のOllamaネットワークに接続
  # ※ "ai-setup"がollamaのフォルダ名の場合
  # ※ 異なる場合は実際のネットワーク名に変更してください
  ai-setup_default:
    external: true
    # フォルダ名が異なる場合の例：
    # ollama-docker_default:
    #   external: true
    # または
    # my-ai-setup_default:
    #   external: true

# ボリューム定義
volumes:
  pip_cache:
    driver: local
  torch_cache:
    driver: local
  huggingface_cache:
    driver: local
